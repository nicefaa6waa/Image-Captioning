{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"widgets":{"application/vnd.jupyter.widget-state+json":{"19354f4821fd4c2799756acc6ed27562":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2360f14fd03b4c5bbab79d1043009241":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3b51bfcd5b354ba19dc971f5b294d940","placeholder":"​","style":"IPY_MODEL_343e942d40584f7b9153c1df9d891f73","value":" 3.34k/3.34k [00:00&lt;00:00, 214kB/s]"}},"2377ceaa574944f3ba43edcfcd6f314e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3bd5f42bc1564feabcb0cc9512dae7b0","placeholder":"​","style":"IPY_MODEL_2d53feb1db6846bf9d59f14a53b72dab","value":"Downloading builder script: 100%"}},"2416ea9236a74acd92f8d4bfdbfe654e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"27fea41269cd4b6e8d43a22c7c081ba6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4ce6d0f874b7430dae0611121cd4c57c","IPY_MODEL_6ce27b7504854ccb833268cdac0c357f","IPY_MODEL_2360f14fd03b4c5bbab79d1043009241"],"layout":"IPY_MODEL_e9e8688d5c174cc4aa013ad797e19ef4"}},"29e623b684bf414fb2cbbb5573b0d8b2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_48afc3a8a63e416c954ee4479d60eb0b","placeholder":"​","style":"IPY_MODEL_dffa027f1cc3498680044cba280893bc","value":"Downloading extra modules: "}},"2a7334b6e00a4a59976f5352d34a36c0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2b801f4f1bf84f928586632604347c3c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_506521ab0b074ab58a668ab1bf2b0b06","placeholder":"​","style":"IPY_MODEL_e6f3b7a1b23f4a4d9f1ef9d3ce47cb57","value":" 5.94k/5.94k [00:00&lt;00:00, 138kB/s]"}},"2d53feb1db6846bf9d59f14a53b72dab":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3274cefa56b94696bf54569e1c25057b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_29e623b684bf414fb2cbbb5573b0d8b2","IPY_MODEL_f53448c9dd9042efa432a3d5d793d7ca","IPY_MODEL_38f0d72343b54846b73f0347f255a81b"],"layout":"IPY_MODEL_bf991ad35f4c4185993cea8a0be27eb5"}},"343e942d40584f7b9153c1df9d891f73":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"38f0d72343b54846b73f0347f255a81b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7df9accbffe44be485a9289d33baf55d","placeholder":"​","style":"IPY_MODEL_2416ea9236a74acd92f8d4bfdbfe654e","value":" 4.07k/? [00:00&lt;00:00, 75.6kB/s]"}},"3b51bfcd5b354ba19dc971f5b294d940":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3bd5f42bc1564feabcb0cc9512dae7b0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"48afc3a8a63e416c954ee4479d60eb0b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4ce6d0f874b7430dae0611121cd4c57c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2a7334b6e00a4a59976f5352d34a36c0","placeholder":"​","style":"IPY_MODEL_19354f4821fd4c2799756acc6ed27562","value":"Downloading extra modules: 100%"}},"4db3b8e76e304d60b4bec21b66268b0e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4f028dfb6ae149b2851f56857a18e07f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"506521ab0b074ab58a668ab1bf2b0b06":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"575dbfa043cc4e14b20ba430e11c0e97":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6395dff562a047a09e7d33b2be92ad07":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6ce27b7504854ccb833268cdac0c357f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4db3b8e76e304d60b4bec21b66268b0e","max":3344,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6395dff562a047a09e7d33b2be92ad07","value":3344}},"7df9accbffe44be485a9289d33baf55d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"93580c83e6954eba981f6170cbf194f5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a8fa74563fe94928bb3f57b3d5b71c7d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_93580c83e6954eba981f6170cbf194f5","max":5937,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ac179d7595d54e338ca035e082a42600","value":5937}},"ac179d7595d54e338ca035e082a42600":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b0887c2d80e3422b9de9f96f9d714a27":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2377ceaa574944f3ba43edcfcd6f314e","IPY_MODEL_a8fa74563fe94928bb3f57b3d5b71c7d","IPY_MODEL_2b801f4f1bf84f928586632604347c3c"],"layout":"IPY_MODEL_575dbfa043cc4e14b20ba430e11c0e97"}},"bf991ad35f4c4185993cea8a0be27eb5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dd264315b20147f1b59ef9bc3c771568":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dffa027f1cc3498680044cba280893bc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e6f3b7a1b23f4a4d9f1ef9d3ce47cb57":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e9e8688d5c174cc4aa013ad797e19ef4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f53448c9dd9042efa432a3d5d793d7ca":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dd264315b20147f1b59ef9bc3c771568","max":1554,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4f028dfb6ae149b2851f56857a18e07f","value":1554}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11949352,"sourceType":"datasetVersion","datasetId":7512368}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Install Dependancies","metadata":{}},{"cell_type":"code","source":"!pip install -U bitsandbytes ultralytics -q","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Uninstall the existing kaggle package\n!pip uninstall -y kaggle\n\n# Install the latest version of kaggle\n!pip install kaggle --upgrade\n\n# Verify the installed version\n!kaggle -v","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!kaggle competitions list --group entered","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create the .kaggle directory if it doesn't exist\n!mkdir -p ~/.kaggle\n\n# Move the uploaded kaggle.json file to ~/.kaggle/\n!cp /kaggle/input/kaggle/kaggle.json ~/.kaggle/\n\n# Set permissions to ensure security (only the user can read/write)\n!chmod 600 ~/.kaggle/kaggle.json","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!kaggle competitions list","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!kaggle competitions download -c obss-intern-competition-2025 -p /kaggle/working/obss-intern-competition-2025","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!unzip /kaggle/working/obss-intern-competition-2025/obss-intern-competition-2025.zip -d /kaggle/working/obss-intern-competition-2025","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# !!!RESTART THE SYSTEM SO bitstandbytes updates","metadata":{}},{"cell_type":"markdown","source":"# Imports and Paths","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\nfrom torch.cuda.amp import autocast, GradScaler\n","metadata":{"id":"4LHNs79jbuuH","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"PATHS = {\n    'train_img_dir': \"/kaggle/working/obss-intern-competition-2025/train/train/\",\n    'train_csv_path': \"/kaggle/working/obss-intern-competition-2025/train.csv\",\n    'test_csv_path': \"/kaggle/working/obss-intern-competition-2025/test.csv\", \n    'test_img_dir': \"/kaggle/working/obss-intern-competition-2025/test/test/\",\n}","metadata":{"id":"5QWmG3e0cCPU","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training Loop","metadata":{}},{"cell_type":"code","source":"from transformers import Blip2ForConditionalGeneration, AutoProcessor\nfrom peft import LoraConfig, get_peft_model, TaskType\nimport torch\nimport torch.nn as nn\nfrom torch.optim import AdamW\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim.lr_scheduler import LambdaLR\nimport time\nfrom tqdm import tqdm\nimport os\nimport shutil\nfrom PIL import Image, ImageEnhance\nimport pandas as pd\nimport random\n\n# GPU setup\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n\n# Check disk space\nworking_dir = \"/kaggle/working\"\ntotal, used, free = shutil.disk_usage(working_dir)\nprint(f\"Disk: Total={total/(1024**3):.2f} GB, Used={used/(1024**3):.2f} GB, Free={free/(1024**3):.2f} GB\")\n\n# Load dataset\ntry:\n    train_df = pd.read_csv(PATHS['train_csv_path'])\n    print(f\"Competition dataset loaded: {len(train_df)} samples\")\n    \n    # Clean the dataset\n    train_df = train_df.dropna(subset=['caption', 'image_id'])\n    train_df = train_df[train_df['caption'].str.len() > 1]\n    print(f\"Cleaned dataset: {len(train_df)} samples\")\nexcept Exception as e:\n    print(f\"Error loading dataset: {e}\")\n    raise ValueError(\"PATHS['train_csv_path'] must be defined and accessible\")\n\n# Load processor\nprint(\"Loading processor...\")\nprocessor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\", use_fast=True)\n\n# Set resolution to match pre-trained model\nprocessor.image_processor.size = {\"height\": 224, \"width\": 224}\nprocessor.image_processor.do_resize = True\nprocessor.image_processor.do_center_crop = False\n\nclass OptimizedCustomDataset(Dataset):\n    def __init__(self, dataframe, processor, img_dir, is_training=True):\n        self.dataframe = dataframe\n        self.processor = processor\n        self.img_dir = img_dir\n        self.is_training = is_training\n        \n    def __len__(self):\n        return len(self.dataframe)\n    \n    def _augment_image(self, image):\n        \"\"\"Optimized data augmentation for faster processing\"\"\"\n        if not self.is_training:\n            return image\n            \n        # Random horizontal flip (40% chance)\n        if random.random() > 0.6:\n            image = image.transpose(Image.FLIP_LEFT_RIGHT)\n        \n        # Random rotation (20% chance, ±10 degrees)\n        if random.random() > 0.8:\n            angle = random.uniform(-10, 10)\n            image = image.rotate(angle, resample=Image.BICUBIC, expand=False)\n        \n        # Brightness adjustment (20% chance)\n        if random.random() > 0.8:\n            enhancer = ImageEnhance.Brightness(image)\n            factor = random.uniform(0.9, 1.1)\n            image = enhancer.enhance(factor)\n            \n        # Color jitter (20% chance)\n        if random.random() > 0.8:\n            enhancer = ImageEnhance.Color(image)\n            factor = random.uniform(0.9, 1.1)\n            image = enhancer.enhance(factor)\n            \n        # Contrast jitter (20% chance)\n        if random.random() > 0.8:\n            enhancer = ImageEnhance.Contrast(image)\n            factor = random.uniform(0.9, 1.1)\n            image = enhancer.enhance(factor)\n            \n        return image\n    \n    def _augment_caption(self, caption):\n        \"\"\"Simplified caption augmentation for faster processing\"\"\"\n        if not self.is_training or random.random() > 0.4:  # Increased from 0.3\n            return caption\n            \n        prefixes = [\n            \"This image shows \",\n            \"The picture displays \",\n            \"In this scene, \",\n            \"\"  # No prefix\n        ]\n        \n        caption_lower = caption.lower()\n        natural_starts = [\"the\", \"this\", \"an\", \"a\", \"in\", \"there\"]\n        \n        if not any(caption_lower.startswith(start) for start in natural_starts):\n            if random.random() > 0.7:\n                prefix = random.choice(prefixes[:-1])\n                caption = prefix + caption.lower()\n            \n        return caption\n    \n    def __getitem__(self, idx):\n        row = self.dataframe.iloc[idx]\n        \n        image_name = str(row['image_id'])\n        if not image_name.endswith(('.jpg', '.jpeg', '.png')):\n            image_name += '.jpg'\n        \n        try:\n            image_path = os.path.join(self.img_dir, image_name)\n        except:\n            image_path = image_name\n        \n        caption = str(row['caption'])\n        \n        # Load and augment image\n        try:\n            image = Image.open(image_path).convert('RGB')\n            image = self._augment_image(image)\n        except Exception as e:\n            print(f\"Error loading image {image_path}: {e}\")\n            image = Image.new('RGB', (224, 224), color='black')\n        \n        # Process image only\n        try:\n            encoding = self.processor(images=image, return_tensors=\"pt\")\n            encoding = {k: v.squeeze(0) for k, v in encoding.items()}\n        except Exception as e:\n            print(f\"Error processing image: {e}\")\n            encoding = {'pixel_values': torch.zeros((3, 224, 224), dtype=torch.float16)}\n        \n        encoding['text'] = self._augment_caption(caption)\n        return encoding\n\ndef optimized_collate_fn(batch):\n    \"\"\"Enhanced collate function with batch-level text processing\"\"\"\n    pixel_values = torch.stack([example['pixel_values'] for example in batch])\n    \n    # Tokenize captions at batch level\n    text_inputs = processor.tokenizer(\n        [example['text'] for example in batch],\n        padding=True,\n        truncation=True,\n        max_length=64,\n        return_tensors=\"pt\",\n        add_special_tokens=True\n    )\n    \n    return {\n        'pixel_values': pixel_values,\n        'input_ids': text_inputs['input_ids'],\n        'attention_mask': text_inputs['attention_mask']\n    }\n\n# Load base model\nmodel_name = \"Salesforce/blip2-opt-2.7b\"\nprint(f\"Loading base model: {model_name}\")\nmodel = Blip2ForConditionalGeneration.from_pretrained(\n    model_name,\n    torch_dtype=torch.float16,\n    device_map={\"\": 0}\n)\n\n# Move model to device\nmodel = model.to(device)\n\n# Generate target modules - target all layers\ntarget_modules = []\nfor i in range(32):  # All 32 layers\n    for proj in [\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\"]:\n        target_modules.append(f\"language_model.model.decoder.layers.{i}.self_attn.{proj}\")\n\nprint(f\"Targeting {len(target_modules)} modules for LoRA\")\n\n# LoRA configuration - increased capacity\nlora_config = LoraConfig(\n    task_type=TaskType.SEQ_2_SEQ_LM,\n    r=24,  # Increased from 16\n    lora_alpha=48,  # Increased from 32\n    lora_dropout=0.1,\n    bias=\"none\",\n    target_modules=target_modules,\n    init_lora_weights=\"gaussian\"\n)\n\n# Apply LoRA with fallback\ntry:\n    model = get_peft_model(model, lora_config)\n    print(\"LoRA applied successfully!\")\nexcept Exception as e:\n    print(f\"LoRA application failed: {e}\")\n    # Minimal fallback - just one layer\n    lora_config = LoraConfig(\n        task_type=TaskType.SEQ_2_SEQ_LM,\n        r=24,\n        lora_alpha=48,\n        lora_dropout=0.1,\n        target_modules=[\"language_model.model.decoder.layers.0.self_attn.q_proj\"],\n        init_lora_weights=\"gaussian\"\n    )\n    model = get_peft_model(model, lora_config)\n    print(\"Fallback LoRA applied successfully!\")\n\n# Initialize LoRA weights conservatively\nfor name, param in model.named_parameters():\n    if \"lora\" in name.lower():\n        if \"lora_A\" in name:\n            nn.init.normal_(param, mean=0.0, std=0.001)\n        elif \"lora_B\" in name:\n            nn.init.zeros_(param)\n        param.requires_grad = True\n        param.data = param.data.to(torch.float32)\n\n# Verify trainable parameters\nmodel.print_trainable_parameters()\n\n# Stable loss function\nclass StableCaptionLoss(nn.Module):\n    def __init__(self, ignore_index=-100):\n        super().__init__()\n        self.loss_fn = nn.CrossEntropyLoss(\n            ignore_index=ignore_index,\n            reduction='mean'\n        )\n    \n    def forward(self, logits, labels):\n        logits = torch.clamp(logits, min=-5, max=5)\n        logits = logits.view(-1, logits.size(-1))\n        labels = labels.view(-1)\n        \n        if torch.any(torch.isnan(logits)) or torch.any(torch.isinf(logits)):\n            print(\"Warning: Invalid logits detected before loss calculation\")\n            return torch.tensor(0.0, device=logits.device, requires_grad=True)\n        \n        loss = self.loss_fn(logits, labels)\n        \n        if torch.isnan(loss) or torch.isinf(loss):\n            print(\"Warning: Invalid loss computed, returning zero\")\n            return torch.tensor(0.0, device=logits.device, requires_grad=True)\n        \n        return loss\n\ncustom_loss_fn = StableCaptionLoss()\n\n# Optimizer with stable learning rate\noptimizer = AdamW(\n    [p for p in model.parameters() if p.requires_grad], \n    lr=1e-4,  # Adjusted from 1.2e-4\n    weight_decay=0.001,\n    eps=1e-8, \n    betas=(0.9, 0.95)\n)\n\n# Linear warmup scheduler\ndef lr_lambda(step):\n    warmup_steps = 100\n    if step < warmup_steps:\n        return float(step) / float(max(1, warmup_steps))\n    return 1.0\n\nscheduler = LambdaLR(optimizer, lr_lambda)\n\nprint(\"Model setup completed!\")\n\n# Create dataset and dataloader\ntry:\n    img_dir = PATHS['train_img_dir']\nexcept:\n    print(f\"Error: PATHS['train_img_dir'] must be defined\")\n    raise ValueError(\"PATHS['train_img_dir'] must be defined and accessible\")\n\ntrain_dataset = OptimizedCustomDataset(\n    train_df, \n    processor, \n    img_dir, \n    is_training=True\n)\n\n# DataLoader with optimized settings\nbatch_size = 10  # Increased from 8\ntrain_dataloader = DataLoader(\n    train_dataset, \n    batch_size=batch_size, \n    shuffle=True, \n    collate_fn=optimized_collate_fn,\n    num_workers=0,\n    pin_memory=True,\n    drop_last=True\n)\n\nprint(f\"Dataset created: {len(train_dataset)} samples\")\nprint(f\"Dataloader created: {len(train_dataloader)} batches\")\n\n# Debug: Check one batch\ntry:\n    batch = next(iter(train_dataloader))\n    print(\"Batch shapes:\", {k: v.shape if hasattr(v, 'shape') else type(v) for k, v in batch.items()})\n    print(\"Sample input_ids range:\", torch.min(batch['input_ids']), \"to\", torch.max(batch['input_ids']))\n    print(\"Sample attention_mask sum:\", torch.sum(batch['attention_mask'], dim=1))\nexcept Exception as e:\n    print(f\"Error checking batch: {e}\")\n\n# Space-efficient model saving functions\ndef save_lora_weights_only(model, filepath, epoch, loss, successful_steps, failed_steps):\n    \"\"\"Save only LoRA adapter weights to minimize file size\"\"\"\n    try:\n        # Extract only LoRA parameters\n        lora_state_dict = {}\n        for name, param in model.named_parameters():\n            if 'lora' in name.lower() and param.requires_grad:\n                lora_state_dict[name] = param.cpu().detach().clone()\n        \n        # Save LoRA weights + metadata\n        torch.save({\n            'lora_weights': lora_state_dict,\n            'lora_config': lora_config.__dict__,  # Save LoRA config for later loading\n            'epoch': epoch,\n            'loss': loss,\n            'successful_steps': successful_steps,\n            'failed_steps': failed_steps,\n            'model_name': model_name  # Base model name for loading later\n        }, filepath)\n        \n        file_size = os.path.getsize(filepath) / (1024**2)  # Size in MB\n        print(f\"LoRA weights saved: {filepath} ({file_size:.1f} MB)\")\n        return True\n    except Exception as e:\n        print(f\"Error saving LoRA weights: {e}\")\n        return False\n\ndef load_and_merge_lora_for_inference(base_model_name, lora_weights_path, device):\n    \"\"\"Helper function to load base model + LoRA weights for inference later\"\"\"\n    # This function is for reference - you'll use it during inference\n    try:\n        # Load base model\n        base_model = Blip2ForConditionalGeneration.from_pretrained(\n            base_model_name,\n            torch_dtype=torch.float16,\n            device_map={\"\": 0}\n        )\n        \n        # Load LoRA checkpoint\n        checkpoint = torch.load(lora_weights_path, map_location=device)\n        lora_config_dict = checkpoint['lora_config']\n        \n        # Recreate LoRA config\n        lora_config = LoraConfig(**lora_config_dict)\n        \n        # Apply LoRA to base model\n        model = get_peft_model(base_model, lora_config)\n        \n        # Load LoRA weights\n        lora_weights = checkpoint['lora_weights']\n        for name, param in model.named_parameters():\n            if name in lora_weights:\n                param.data = lora_weights[name].to(device)\n        \n        print(f\"Model loaded with LoRA weights from {lora_weights_path}\")\n        return model\n    except Exception as e:\n        print(f\"Error loading model with LoRA weights: {e}\")\n        return None\n\n# Enhanced training step with better error handling\ndef stable_train_step(model, batch, device, custom_loss_fn):\n    try:\n        # Move inputs to device with proper dtypes\n        pixel_values = batch[\"pixel_values\"].to(device, dtype=torch.float16, non_blocking=True)\n        input_ids = batch[\"input_ids\"].to(device, non_blocking=True)\n        attention_mask = batch[\"attention_mask\"].to(device, non_blocking=True)\n        \n        # Validate inputs\n        if torch.any(torch.isnan(pixel_values)) or torch.any(torch.isinf(pixel_values)):\n            print(\"Warning: Invalid pixel values detected\")\n            return None\n        \n        # Check token range (OPT vocab size is ~50272)\n        if torch.any(input_ids < 0) or torch.any(input_ids >= 50272):\n            print(f\"Warning: Invalid input_ids detected, range: {torch.min(input_ids)} to {torch.max(input_ids)}\")\n            # Clamp to valid range\n            input_ids = torch.clamp(input_ids, 0, 50271)\n        \n        # Create labels\n        labels = input_ids.clone()\n        labels[attention_mask == 0] = -100\n        \n        # Forward pass with error handling\n        model.train()\n        \n        # Get the base model for forward pass\n        if hasattr(model, 'base_model'):\n            base_model = model.base_model\n        else:\n            base_model = model\n        \n        # Use a more stable forward pass approach\n        with torch.backends.cudnn.flags(enabled=False):\n            outputs = base_model(\n                pixel_values=pixel_values,\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                labels=labels,\n                return_dict=True,\n                use_cache=False,\n                output_hidden_states=False,\n                output_attentions=False\n            )\n        \n        # Calculate loss\n        if outputs.loss is not None and not (torch.isnan(outputs.loss) or torch.isinf(outputs.loss)):\n            loss = outputs.loss\n        else:\n            if outputs.logits is not None:\n                loss = custom_loss_fn(outputs.logits, labels)\n            else:\n                print(\"No logits in outputs\")\n                return None\n        \n        # Final validation\n        if torch.isnan(loss) or torch.isinf(loss) or loss.item() > 20.0:\n            print(f\"Invalid or extreme loss: {loss.item()}\")\n            return None\n        \n        return loss\n        \n    except RuntimeError as e:\n        if \"out of memory\" in str(e):\n            print(\"CUDA out of memory, clearing cache...\")\n            torch.cuda.empty_cache()\n        else:\n            print(f\"Runtime error in forward pass: {e}\")\n        return None\n    except Exception as e:\n        print(f\"Unexpected error in forward pass: {e}\")\n        return None\n\n# Training parameters - REDUCED TO 2 EPOCHS\naccumulation_steps = 3  # Adjusted from 4\nnum_epochs = 2  # REDUCED FROM 3 TO SAVE SPACE AND TIME\nprint_every = 50\n\nprint(f\"Starting stable LoRA BLIP-2 training...\")\nprint(f\"Effective batch size: {batch_size * accumulation_steps}\")\nprint(f\"Total steps per epoch: {len(train_dataloader)}\")\nprint(f\"Effective steps per epoch: {len(train_dataloader) // accumulation_steps}\")\nprint(f\"TRAINING FOR {num_epochs} EPOCHS TO SAVE SPACE\")\n\n# Training loop\nmodel.train()\ntotal_loss = 0\nstep = 0\nsuccessful_steps = 0\nfailed_steps = 0\ngrad_accum_counter = 0\n\n# Add gradient clipping hook for extra safety\ndef grad_hook(grad):\n    return torch.clamp(grad, -1.0, 1.0)\n\n# Register hooks on LoRA parameters\nhooks = []\nfor name, param in model.named_parameters():\n    if \"lora\" in name.lower() and param.requires_grad:\n        hooks.append(param.register_hook(grad_hook))\n\ntry:\n    for epoch in range(num_epochs):\n        epoch_loss = 0\n        progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n        \n        for i, batch in enumerate(progress_bar):\n            # Get loss\n            loss = stable_train_step(model, batch, device, custom_loss_fn)\n            \n            if loss is None:\n                failed_steps += 1\n                optimizer.zero_grad()\n                grad_accum_counter = 0\n                torch.cuda.empty_cache()\n                \n                progress_bar.set_postfix({\n                    'Status': 'FAILED',\n                    'Fails': failed_steps,\n                    'Success': successful_steps\n                })\n                continue\n            \n            # Scale loss for accumulation\n            loss = loss / accumulation_steps\n            \n            # Backward pass\n            try:\n                loss.backward()\n                grad_accum_counter += 1\n                \n                if grad_accum_counter == accumulation_steps:\n                    valid_gradients = True\n                    grad_count = 0\n                    for param in model.parameters():\n                        if param.grad is not None:\n                            grad_count += 1\n                            if torch.any(torch.isnan(param.grad)) or torch.any(torch.isinf(param.grad)):\n                                valid_gradients = False\n                                break\n                    \n                    if valid_gradients and grad_count > 0:\n                        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n                        optimizer.step()\n                        optimizer.zero_grad()\n                        step += 1\n                        successful_steps += 1\n                        scheduler.step()  # Update learning rate\n                    else:\n                        print(f\"Invalid gradients detected (valid: {valid_gradients}, count: {grad_count}), skipping step\")\n                        optimizer.zero_grad()\n                        failed_steps += 1\n                    \n                    grad_accum_counter = 0\n                \n                # Update metrics\n                loss_value = loss.item() * accumulation_steps\n                total_loss += loss_value\n                epoch_loss += loss_value\n                \n                if successful_steps > 0:\n                    avg_loss = total_loss / successful_steps\n                    success_rate = successful_steps / (successful_steps + failed_steps) * 100\n                    progress_bar.set_postfix({\n                        'Loss': f'{loss_value:.4f}',\n                        'Avg': f'{avg_loss:.4f}',\n                        'Success': f'{success_rate:.1f}%'\n                    })\n                \n                if (i + 1) % print_every == 0:\n                    success_rate = successful_steps / (successful_steps + failed_steps) * 100 if (successful_steps + failed_steps) > 0 else 0\n                    avg_loss = total_loss / successful_steps if successful_steps > 0 else 0\n                    print(f\"Step {step}: Loss = {loss_value:.4f}, Avg = {avg_loss:.4f}, Success Rate = {success_rate:.1f}%\")\n                \n            except Exception as e:\n                print(f\"Error in backward pass: {e}\")\n                optimizer.zero_grad()\n                grad_accum_counter = 0\n                failed_steps += 1\n                torch.cuda.empty_cache()\n                continue\n        \n        # Epoch summary and space-efficient saving\n        if successful_steps > 0:\n            avg_loss = epoch_loss / successful_steps\n            success_rate = successful_steps / (successful_steps + failed_steps) * 100\n            print(f\"Epoch {epoch+1} completed:\")\n            print(f\"  Average Loss: {avg_loss:.4f}\")\n            print(f\"  Success Rate: {success_rate:.1f}% ({successful_steps}/{successful_steps + failed_steps})\")\n            \n            # Save only LoRA weights (much smaller!)\n            model_path = f\"/kaggle/working/blip2_lora_epoch_{epoch+1}.pt\"\n            save_success = save_lora_weights_only(\n                model, model_path, epoch + 1, avg_loss, successful_steps, failed_steps\n            )\n            \n            if save_success:\n                # Check disk space after saving\n                total, used, free = shutil.disk_usage(working_dir)\n                print(f\"Disk after save: Free={free/(1024**3):.2f} GB\")\n            else:\n                print(f\"Failed to save model for epoch {epoch+1}\")\n        else:\n            print(f\"Epoch {epoch+1} had no successful steps!\")\n\nfinally:\n    # Clean up hooks\n    for hook in hooks:\n        hook.remove()\n\nsuccess_rate = successful_steps / (successful_steps + failed_steps) * 100 if (successful_steps + failed_steps) > 0 else 0\nprint(f\"Training completed!\")\nprint(f\"Final success rate: {success_rate:.1f}% ({successful_steps}/{successful_steps + failed_steps})\")\nprint(f\"Total successful steps: {successful_steps}\")\n\n# Save final LoRA weights if we had any successful steps\nif successful_steps > 0:\n    final_model_path = \"/kaggle/working/blip2_lora_final.pt\"\n    final_avg_loss = total_loss / successful_steps\n    save_success = save_lora_weights_only(\n        model, final_model_path, num_epochs, final_avg_loss, successful_steps, failed_steps\n    )\n    \n    if save_success:\n        print(f\"Final LoRA weights saved: {final_model_path}\")\n    else:\n        print(\"Failed to save final LoRA weights\")\n\nprint(\"Saved files:\", [f for f in os.listdir(\"/kaggle/working\") if f.endswith('.pt')])\n\n# Print instructions for loading the model later\nprint(\"\\n\" + \"=\"*50)\nprint(\"IMPORTANT: How to load your trained model for inference:\")\nprint(\"=\"*50)\nprint(\"1. Use the load_and_merge_lora_for_inference() function provided above\")\nprint(\"2. Example usage:\")\nprint(\"   model = load_and_merge_lora_for_inference(\")\nprint(\"       'Salesforce/blip2-opt-2.7b',\")\nprint(\"       '/path/to/blip2_lora_final.pt',\")\nprint(\"       device\")\nprint(\"   )\")\nprint(\"3. Then use the model normally for generating captions\")\nprint(\"=\"*50)","metadata":{"id":"HwuYiqLDgFq2","outputId":"20cfe566-d18a-4903-a06b-40920e4b2881","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Test Model Eval","metadata":{}},{"cell_type":"markdown","source":"# !! RESTART TO CLEAR VRAM AND USE LOCALLY SAVED MODEL","metadata":{}},{"cell_type":"code","source":"# Install ultralytics for YOLOv5 and nltk for post-processing\n!pip install ultralytics","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\nfrom torch.cuda.amp import autocast, GradScaler","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"PATHS = {\n    'train_img_dir': \"/kaggle/working/obss-intern-competition-2025/train/train/\",\n    'train_csv_path': \"/kaggle/working/obss-intern-competition-2025/train.csv\",\n    'test_csv_path': \"/kaggle/working/obss-intern-competition-2025/test.csv\", \n    'test_img_dir': \"/kaggle/working/obss-intern-competition-2025/test/test/\",\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport re\nfrom transformers import Blip2ForConditionalGeneration, AutoProcessor, BitsAndBytesConfig\nfrom peft import PeftModel\nimport gc\nimport os\nimport pandas as pd\nimport random\nimport nltk\nfrom ultralytics import YOLO\nimport numpy as np\n\n# Download nltk data (run once)\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\nnltk.download('averaged_perceptron_tagger_eng')\n\n# Updated model paths for new training format\nsaved_model_paths = [\n    \"/kaggle/working/stable_blip2_lora_final.pt\",\n    \"/kaggle/working/stable_blip2_lora_epoch_3.pt\",\n    \"/kaggle/working/stable_blip2_lora_epoch_2.pt\",\n    \"/kaggle/working/stable_blip2_lora_epoch_1.pt\"\n]\n\n# Enhanced YOLO model loader with better error handling\ndef load_yolo_model():\n    \"\"\"Load YOLOv11 model with enhanced capabilities\"\"\"\n    try:\n        yolo_model = YOLO(\"yolo11n.pt\")\n        print(\"✅ YOLOv11n model loaded successfully\")\n        return yolo_model\n    except Exception as e:\n        print(f\"⚠️ Failed to load YOLOv11n, trying YOLOv11s: {e}\")\n        try:\n            yolo_model = YOLO(\"yolo11s.pt\")\n            print(\"✅ YOLOv11s model loaded successfully\")\n            return yolo_model\n        except Exception as e2:\n            print(f\"⚠️ Failed to load any YOLO model: {e2}. Proceeding without object detection.\")\n            return None\n\nyolo_model = load_yolo_model()\n\ndef get_enhanced_object_detection(image, yolo_model):\n    \"\"\"Enhanced object detection with confidence filtering and spatial awareness\"\"\"\n    if yolo_model is None:\n        return None, []\n    \n    try:\n        results = yolo_model(image, conf=0.3, iou=0.5)  # Adjusted thresholds\n        \n        if len(results) == 0 or len(results[0].boxes) == 0:\n            return None, []\n        \n        # Extract detection info\n        boxes = results[0].boxes\n        detected_objects = []\n        spatial_info = {}\n        \n        img_width, img_height = image.size\n        \n        for i, box in enumerate(boxes):\n            cls_id = int(box.cls.cpu().numpy())\n            confidence = float(box.conf.cpu().numpy())\n            class_name = results[0].names[cls_id]\n            \n            # Get bounding box coordinates\n            x1, y1, x2, y2 = box.xyxy.cpu().numpy()[0]\n            \n            # Calculate spatial properties\n            center_x = (x1 + x2) / 2 / img_width\n            center_y = (y1 + y2) / 2 / img_height\n            area_ratio = ((x2 - x1) * (y2 - y1)) / (img_width * img_height)\n            \n            detected_objects.append({\n                'class': class_name,\n                'confidence': confidence,\n                'center_x': center_x,\n                'center_y': center_y,\n                'area_ratio': area_ratio,\n                'position': get_spatial_position(center_x, center_y)\n            })\n        \n        # Group by class and get dominant objects\n        class_counts = {}\n        for obj in detected_objects:\n            class_name = obj['class']\n            if class_name not in class_counts:\n                class_counts[class_name] = []\n            class_counts[class_name].append(obj)\n        \n        # Get spatial context\n        spatial_context = analyze_spatial_context(detected_objects, img_width, img_height)\n        \n        return detected_objects, spatial_context\n        \n    except Exception as e:\n        print(f\"⚠️ Object detection failed: {e}\")\n        return None, []\n\ndef get_spatial_position(center_x, center_y):\n    \"\"\"Determine spatial position in image\"\"\"\n    if center_x < 0.33:\n        h_pos = \"left\"\n    elif center_x > 0.67:\n        h_pos = \"right\"\n    else:\n        h_pos = \"center\"\n    \n    if center_y < 0.33:\n        v_pos = \"top\"\n    elif center_y > 0.67:\n        v_pos = \"bottom\"\n    else:\n        v_pos = \"middle\"\n    \n    if h_pos == \"center\" and v_pos == \"middle\":\n        return \"center\"\n    return f\"{v_pos}-{h_pos}\"\n\ndef analyze_spatial_context(detected_objects, img_width, img_height):\n    \"\"\"Analyze spatial relationships between objects\"\"\"\n    context = {\n        'dominant_objects': [],\n        'scene_type': 'general',\n        'object_relationships': []\n    }\n    \n    if not detected_objects:\n        return context\n    \n    # Sort by area (largest first)\n    sorted_objects = sorted(detected_objects, key=lambda x: x['area_ratio'], reverse=True)\n    \n    # Get dominant objects (top 3 by area)\n    context['dominant_objects'] = [obj['class'] for obj in sorted_objects[:3]]\n    \n    # Determine scene type\n    classes = [obj['class'] for obj in detected_objects]\n    if any(cls in ['person', 'sports ball', 'baseball bat', 'baseball glove'] for cls in classes):\n        context['scene_type'] = 'sports'\n    elif any(cls in ['car', 'truck', 'bus', 'motorcycle'] for cls in classes):\n        context['scene_type'] = 'traffic'\n    elif any(cls in ['dining table', 'chair', 'cup', 'bowl'] for cls in classes):\n        context['scene_type'] = 'indoor'\n    elif any(cls in ['tree', 'grass', 'sky'] for cls in classes):\n        context['scene_type'] = 'outdoor'\n    \n    return context\n\n# FIXED: Enhanced model loader with proper memory management\ndef load_trained_model(force_reload=False):\n    \"\"\"Load the trained model with LoRA weights - MEMORY OPTIMIZED (FP16, no quantization)\"\"\"\n    global model, processor\n    \n    # Avoid reloading if model is already functional\n    if not force_reload and 'model' in globals() and model is not None:\n        try:\n            dummy_tensor = torch.randn(1, 3, 224, 224).to(device, torch.float16)\n            with torch.no_grad():\n                _ = model.generate(pixel_values=dummy_tensor, max_length=5)\n            print(\"✅ Using existing model from VRAM\")\n            return model, processor\n        except Exception as e:\n            print(f\"⚠️ Model in VRAM not functional: {e}\")\n            if 'model' in globals():\n                del model\n            if 'processor' in globals():\n                del processor\n            gc.collect()\n            torch.cuda.empty_cache()\n    \n    # Find the best available model\n    model_path = None\n    for path in saved_model_paths:\n        if os.path.exists(path):\n            model_path = path\n            print(f\"Found model: {model_path}\")\n            break\n    \n    if model_path is None:\n        raise FileNotFoundError(\"No trained model found. Please check the model paths.\")\n    \n    try:\n        print(f\"🔄 Loading processor...\")\n        processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n        processor.image_processor.size = {\"height\": 224, \"width\": 224}\n        \n        print(f\"🔄 Loading checkpoint to CPU first...\")\n        checkpoint = torch.load(model_path, map_location='cpu')\n        \n        print(f\"🔄 Loading base model in FP16...\")\n        # Load base model in FP16 without quantization\n        base_model = Blip2ForConditionalGeneration.from_pretrained(\n            \"Salesforce/blip2-opt-2.7b\",\n            torch_dtype=torch.float16,\n            device_map=\"auto\",\n            low_cpu_mem_usage=True,\n            max_memory={0: \"12GB\"}  # Limit GPU memory\n        )\n        print(\"✅ Base model loaded in FP16\")\n        \n        # Apply LoRA configuration (matching training code)\n        from peft import LoraConfig, get_peft_model, TaskType\n        \n        target_modules = []\n        for i in range(32):  # Match training configuration\n            for proj in [\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\"]:\n                target_modules.append(f\"language_model.model.decoder.layers.{i}.self_attn.{proj}\")\n        \n        lora_config = LoraConfig(\n            task_type=TaskType.SEQ_2_SEQ_LM,\n            r=24,  # Match training\n            lora_alpha=48,  # Match training\n            lora_dropout=0.1,\n            bias=\"none\",\n            target_modules=target_modules,\n            init_lora_weights=\"gaussian\"\n        )\n        \n        try:\n            model = get_peft_model(base_model, lora_config)\n            \n            # Load LoRA weights\n            state_dict = checkpoint.get('lora_weights', checkpoint.get('model_state_dict'))\n            if not state_dict:\n                print(\"⚠️ No LoRA weights found in checkpoint\")\n                model = base_model\n            else:\n                # Only load compatible keys\n                compatible_state_dict = {k: v for k, v in state_dict.items() if k in model.state_dict()}\n                model.load_state_dict(compatible_state_dict, strict=False)\n                print(f\"✅ Loaded {len(compatible_state_dict)} LoRA parameters\")\n        \n        except Exception as e:\n            print(f\"⚠️ LoRA loading failed, using base model: {e}\")\n            model = base_model\n        \n        # Clean up checkpoint\n        del checkpoint\n        gc.collect()\n        torch.cuda.empty_cache()\n        \n        model = model.to(device)\n        model.eval()\n        \n        print(\"✅ Model loaded successfully\")\n        \n        # Print memory usage\n        if torch.cuda.is_available():\n            memory_used = torch.cuda.memory_allocated() / 1024**3\n            print(f\"📊 GPU Memory Used: {memory_used:.2f}GB\")\n        \n        return model, processor\n        \n    except Exception as e:\n        print(f\"❌ Failed to load model: {e}\")\n        for var_name in ['model', 'base_model', 'checkpoint']:\n            if var_name in locals():\n                del locals()[var_name]\n        gc.collect()\n        torch.cuda.empty_cache()\n        raise\n\ndef clean_caption(caption):\n    \"\"\"Enhanced caption cleaning\"\"\"\n    if not caption:\n        return \"An image showing a scene\"\n    \n    # Remove HTML tags and special markers\n    caption = re.sub(r'<[^>]*>', '', caption)\n    caption = re.sub(r'\\[.*?\\]', '', caption)\n    caption = re.sub(r'[\\x00-\\x1F\\x7F-\\x9F]', '', caption)\n    caption = re.sub(r'�|', '', caption)\n    \n    # Remove problematic tokens\n    problematic_tokens = [\n        'strutconnect', 'strutconnector', 'attrot', 'guiactive', 'guiactiveunfocused',\n        'madeupword', 'confignode', 'partmodule', 'tweakscale', 'modulemanager',\n        'gamedata', 'squad', 'parttools', 'kspfield', 'persistant', 'cfgnode',\n        'guiicon', 'guiname', '0002', 'externaltoevaonly', '裏', 'guiactiveunfocusedmadeupword'\n    ]\n    \n    for token in problematic_tokens:\n        caption = re.sub(rf'\\b{token}\\b', '', caption, flags=re.IGNORECASE)\n    \n    # Remove camelCase technical terms\n    caption = re.sub(r'\\b[A-Z][a-z]*[A-Z][a-zA-Z]*\\b', '', caption)\n    caption = re.sub(r'\\b\\w+000\\d+\\b', '', caption)\n    \n    # Clean up spacing and grammar\n    caption = re.sub(r'\\s+', ' ', caption).strip()\n    caption = re.sub(r'\\s+(and|or|the|a|an)$', '', caption, flags=re.IGNORECASE)\n    caption = re.sub(r'^(the|a|an)\\s+', '', caption, flags=re.IGNORECASE)\n    \n    # Capitalize first letter\n    if caption and len(caption) > 0:\n        caption = caption[0].upper() + caption[1:] if len(caption) > 1 else caption.upper()\n    \n    # Fallback for empty or problematic captions\n    if len(caption.split()) < 2 or not caption or any(t in caption.lower() for t in problematic_tokens):\n        caption = \"An image showing a scene with visible objects\"\n    \n    return caption\n\ndef enhance_caption_with_context(caption, detected_objects, spatial_context):\n    \"\"\"Enhance caption using object detection context\"\"\"\n    if not detected_objects:\n        return caption\n    \n    # Get dominant objects\n    dominant_classes = [obj['class'] for obj in detected_objects[:3]]\n    \n    # Scene-specific enhancements\n    if spatial_context['scene_type'] == 'sports':\n        if 'person' in dominant_classes and any(sports in dominant_classes for sports in ['sports ball', 'baseball bat']):\n            if 'player' not in caption.lower():\n                caption = caption.replace('person', 'player').replace('people', 'players')\n        \n        if 'baseball' in dominant_classes or 'sports ball' in dominant_classes:\n            if 'baseball' not in caption.lower():\n                caption += ' in a baseball context'\n    \n    # Add spatial context if missing\n    positions = [obj['position'] for obj in detected_objects]\n    if any('center' in pos for pos in positions) and 'center' not in caption.lower():\n        caption += ' with a central focus'\n    \n    return caption\n\ndef generate_multiple_captions(model, processor, inputs, image, device, detected_objects=None, spatial_context=None):\n    \"\"\"Generate 6 different caption variants as specified\"\"\"\n    \n    # YOUR ORIGINAL 6 CAPTION CONFIGS\n    caption_configs = [\n        {\n            'name': 'High Quality Detailed',\n            'max_length': 80,\n            'num_beams': 8,\n            'num_beam_groups': 2,\n            'do_sample': False,\n            'length_penalty': 1.5,\n            'no_repeat_ngram_size': 3,\n            'diversity_penalty': 0.7,\n            'early_stopping': True\n        },\n        {\n            'name': 'Creative Sampling',\n            'max_length': 70,\n            'num_beams': 1,\n            'do_sample': True,\n            'top_k': 50,\n            'top_p': 0.9,\n            'temperature': 0.8,\n            'no_repeat_ngram_size': 2,\n            'length_penalty': 1.2\n        },\n        {\n            'name': 'Balanced Precision',\n            'max_length': 60,\n            'num_beams': 6,\n            'do_sample': False,\n            'length_penalty': 1.3,\n            'no_repeat_ngram_size': 2,\n            'early_stopping': True,\n            'repetition_penalty': 1.1\n        },\n        {\n            'name': 'Nucleus Sampling',\n            'max_length': 75,\n            'num_beams': 1,\n            'do_sample': True,\n            'top_p': 0.95,\n            'temperature': 0.7,\n            'no_repeat_ngram_size': 2,\n            'length_penalty': 1.0,\n            'repetition_penalty': 1.05\n        },\n        {\n            'name': 'Conservative Beam',\n            'max_length': 50,\n            'num_beams': 4,\n            'do_sample': False,\n            'length_penalty': 1.0,\n            'no_repeat_ngram_size': 2,\n            'early_stopping': True\n        },\n        {\n            'name': 'Training Compatible',\n            'max_length': 64,\n            'num_beams': 4,\n            'do_sample': False,\n            'length_penalty': 1.0,\n            'no_repeat_ngram_size': 2,\n            'early_stopping': True,\n            'pad_token_id': processor.tokenizer.eos_token_id\n        }\n    ]\n    \n    generated_captions = []\n    best_caption = \"An image showing a scene\"\n    best_score = -float('inf')\n    \n    print(f\"🎯 Detected objects: {[obj['class'] for obj in detected_objects] if detected_objects else 'None'}\")\n    print(f\"🎯 Scene type: {spatial_context['scene_type'] if spatial_context else 'general'}\")\n    \n    for config in caption_configs:\n        try:\n            with torch.no_grad():\n                # Generate caption\n                generated_ids = model.generate(\n                    pixel_values=inputs.pixel_values,\n                    **{k: v for k, v in config.items() if k != 'name'}\n                )\n                \n                caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n                cleaned_caption = clean_caption(caption)\n                \n                # Apply context enhancement\n                if detected_objects and spatial_context:\n                    enhanced_caption = enhance_caption_with_context(cleaned_caption, detected_objects, spatial_context)\n                else:\n                    enhanced_caption = cleaned_caption\n                \n                # Score the caption\n                score = score_caption(enhanced_caption, detected_objects, spatial_context)\n                \n                generated_captions.append({\n                    'config': config['name'],\n                    'caption': enhanced_caption,\n                    'score': score,\n                    'length': len(enhanced_caption.split())\n                })\n                \n                print(f\"  {config['name']}: '{enhanced_caption}' (Score: {score:.2f})\")\n                \n                if score > best_score:\n                    best_score = score\n                    best_caption = enhanced_caption\n                    \n        except Exception as e:\n            print(f\"  {config['name']} failed: {e}\")\n            generated_captions.append({\n                'config': config['name'],\n                'caption': \"Generation failed\",\n                'score': -1,\n                'length': 0\n            })\n    \n    return generated_captions, best_caption\n\ndef score_caption(caption, detected_objects, spatial_context):\n    \"\"\"Score caption quality based on various factors\"\"\"\n    score = 0\n    caption_lower = caption.lower()\n    \n    # Base quality score\n    word_count = len(caption.split())\n    if 5 <= word_count <= 15:\n        score += 10\n    elif 3 <= word_count <= 20:\n        score += 5\n    \n    # Grammar and structure\n    if caption[0].isupper():\n        score += 2\n    if not caption.endswith('.'):\n        score += 1  # Prefer captions without periods for this task\n    \n    # Context matching\n    if detected_objects:\n        detected_classes = [obj['class'] for obj in detected_objects]\n        \n        # Sports context\n        if any(cls in ['person', 'sports ball', 'baseball bat'] for cls in detected_classes):\n            if any(word in caption_lower for word in ['baseball', 'player', 'sport', 'game']):\n                score += 15\n            if any(word in caption_lower for word in ['uniform', 'field', 'bat', 'ball']):\n                score += 10\n        \n        # Object presence matching\n        for obj in detected_classes:\n            obj_lower = obj.lower()\n            if obj_lower in caption_lower or any(syn in caption_lower for syn in get_synonyms(obj_lower)):\n                score += 5\n    \n    # Scene type matching\n    if spatial_context:\n        scene_type = spatial_context['scene_type']\n        if scene_type == 'sports' and any(word in caption_lower for word in ['playing', 'game', 'sport', 'field']):\n            score += 8\n        elif scene_type == 'outdoor' and any(word in caption_lower for word in ['outside', 'outdoor', 'field']):\n            score += 5\n    \n    # Penalize generic or problematic phrases\n    generic_phrases = ['an image', 'a picture', 'this shows', 'visible objects']\n    for phrase in generic_phrases:\n        if phrase in caption_lower:\n            score -= 5\n    \n    return score\n\ndef get_synonyms(word):\n    \"\"\"Get simple synonyms for common objects\"\"\"\n    synonyms = {\n        'person': ['man', 'woman', 'player', 'individual'],\n        'sports ball': ['ball', 'baseball'],\n        'baseball bat': ['bat'],\n        'car': ['vehicle', 'automobile'],\n        'truck': ['vehicle']\n    }\n    return synonyms.get(word, [])\n\ndef process_and_display_image(model, processor, sample_row, img_dir, device, idx):\n    \"\"\"Process single image with enhanced caption generation\"\"\"\n    sample_image_name = str(sample_row['image_id'])\n    if not sample_image_name.endswith(('.jpg', '.jpeg', '.png')):\n        sample_image_name += '.jpg'\n    \n    sample_image_path = os.path.join(img_dir, sample_image_name)\n    sample_image = Image.open(sample_image_path).convert('RGB')\n    \n    print(f\"\\n{'='*60}\")\n    print(f\"Processing image {idx+1}: {sample_image_name}\")\n    print(f\"Image size: {sample_image.size}\")\n    \n    # Enhanced object detection\n    detected_objects, spatial_context = get_enhanced_object_detection(sample_image, yolo_model)\n    \n    # Prepare inputs (matching training resolution)\n    inputs = processor(\n        images=sample_image,\n        return_tensors=\"pt\"\n    ).to(device, torch.float16)\n    \n    # Generate multiple captions\n    generated_captions, best_caption = generate_multiple_captions(\n        model, processor, inputs, sample_image, device, detected_objects, spatial_context\n    )\n    \n    # Print detailed results FIRST (as you wanted)\n    print(f\"\\n📊 Caption Generation Results:\")\n    for cap in generated_captions:\n        print(f\"  {cap['config']:20} | Score: {cap['score']:6.2f} | Words: {cap['length']:2} | {cap['caption']}\")\n    \n    print(f\"\\n🏆 Best Caption: {best_caption}\")\n    \n    # Then display image BELOW the captions (as you specified)\n    plt.figure(figsize=(12, 8))\n    plt.imshow(sample_image)\n    plt.axis('off')\n    plt.title(f\"Image {idx+1} - {sample_image_name}\\nBest Caption: {best_caption}\", \n              fontsize=12, wrap=True, pad=20)\n    plt.tight_layout()\n    plt.show()\n    \n    print(f\"{'='*60}\")\n    \n    return {\n        \"image_id\": sample_image_name,\n        \"best_caption\": best_caption,\n        \"all_captions\": generated_captions,\n        \"detected_objects\": [obj['class'] for obj in detected_objects] if detected_objects else []\n    }\n\ndef process_multiple_images(model, processor, test_df, img_dir, device, num_images=10):\n    \"\"\"Process 10 images as specified\"\"\"\n    random_indices = random.sample(range(len(test_df)), min(num_images, len(test_df)))\n    random_rows = test_df.iloc[random_indices]\n    \n    results = []\n    \n    for idx, (_, sample_row) in enumerate(random_rows.iterrows()):\n        result = process_and_display_image(model, processor, sample_row, img_dir, device, idx)\n        results.append(result)\n        \n        # Clear some memory between images\n        torch.cuda.empty_cache()\n        \n        if idx < len(random_rows) - 1:  # Don't wait after the last image\n            input(\"Press Enter to continue to the next image...\")\n    \n    # Final summary\n    print(\"\\n\" + \"=\"*80)\n    print(\"FINAL EVALUATION SUMMARY\")\n    print(\"=\"*80)\n    \n    for i, result in enumerate(results, 1):\n        print(f\"Image {i:2} - {result['image_id']:15} | Objects: {result['detected_objects']}\")\n        print(f\"         Best: {result['best_caption']}\")\n        print()\n    \n    # Statistics\n    total_words = sum(len(result['best_caption'].split()) for result in results)\n    avg_words = total_words / len(results)\n    \n    all_detected_objects = []\n    for result in results:\n        all_detected_objects.extend(result['detected_objects'])\n    \n    unique_objects = list(set(all_detected_objects))\n    \n    print(f\"📈 Statistics:\")\n    print(f\"   Average caption length: {avg_words:.1f} words\")\n    print(f\"   Total unique objects detected: {len(unique_objects)}\")\n    print(f\"   Most common objects: {sorted(set(all_detected_objects), key=all_detected_objects.count, reverse=True)[:5]}\")\n    print(\"=\"*80)\n    \n    return results\n\n# Initialize device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# FIXED: Set memory optimization environment variable\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n\n# Load the trained model\nprint(\"Loading trained model...\")\nmodel, processor = load_trained_model()\n\n# Load test dataset\ntest_df = pd.read_csv(PATHS['test_csv_path'])\nprint(f\"Test dataset loaded: {len(test_df)} samples\")\n\n# Process multiple images (YOUR ORIGINAL FLOW)\nprint(\"Starting enhanced evaluation...\")\nresults = process_multiple_images(model, processor, test_df, PATHS['test_img_dir'], device, num_images=10)","metadata":{"id":"kN-iOf8s28LW","outputId":"ac3c2320-e65a-4039-d4d0-34ac6f50cad4","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Save to submission.csv","metadata":{}},{"cell_type":"markdown","source":"!! RESTART THE CODEBASE TO FREE VRAM ","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\nfrom torch.cuda.amp import autocast, GradScaler","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"PATHS = {\n    'train_img_dir': \"/kaggle/working/obss-intern-competition-2025/train/train/\",\n    'train_csv_path': \"/kaggle/working/obss-intern-competition-2025/train.csv\",\n    'test_csv_path': \"/kaggle/working/obss-intern-competition-2025/test.csv\", \n    'test_img_dir': \"/kaggle/working/obss-intern-competition-2025/test/test/\",\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom PIL import Image\nimport pandas as pd\nimport os\nimport re\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\nfrom transformers import Blip2ForConditionalGeneration, AutoProcessor\nfrom peft import PeftModel, LoraConfig, get_peft_model, TaskType\nimport gc\nimport warnings\nimport logging\n\n# Suppress warnings\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\nos.environ[\"XLA_FLAGS\"] = \"--xla_gpu_cuda_data_dir=/usr/local/cuda\"\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module=\"transformers\")\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nlogging.getLogger(\"ultralytics\").setLevel(logging.ERROR)\n\n\n# Model paths\nsaved_model_paths = [\n    \"/kaggle/working/stable_blip2_lora_final.pt\",\n    \"/kaggle/working/stable_blip2_lora_epoch_3.pt\",\n    \"/kaggle/working/stable_blip2_lora_epoch_2.pt\",\n    \"/kaggle/working/stable_blip2_lora_epoch_1.pt\"\n]\n\n# Model loading\ndef load_trained_model(force_reload=False):\n    \"\"\"Load the trained model with LoRA weights - MEMORY OPTIMIZED (FP16)\"\"\"\n    global model, processor\n    \n    if not force_reload and 'model' in globals() and model is not None:\n        try:\n            dummy_tensor = torch.randn(1, 3, 224, 224).to(device, torch.float16)\n            with torch.no_grad():\n                _ = model.generate(pixel_values=dummy_tensor, max_length=5)\n            print(\"✅ Using existing model from VRAM\")\n            return model, processor\n        except Exception as e:\n            print(f\"⚠️ Model in VRAM not functional: {e}\")\n            if 'model' in globals():\n                del model\n            if 'processor' in globals():\n                del processor\n            gc.collect()\n            torch.cuda.empty_cache()\n    \n    model_path = None\n    for path in saved_model_paths:\n        if os.path.exists(path):\n            model_path = path\n            print(f\"Found model: {model_path}\")\n            break\n    \n    if model_path is None:\n        raise FileNotFoundError(\"No trained model found. Please check the model paths.\")\n    \n    try:\n        print(f\"🔄 Loading processor...\")\n        processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\", use_fast=True)\n        processor.image_processor.size = {\"height\": 224, \"width\": 224}\n        \n        print(f\"🔄 Loading checkpoint to CPU first...\")\n        checkpoint = torch.load(model_path, map_location='cpu')\n        \n        print(f\"🔄 Loading base model in FP16...\")\n        base_model = Blip2ForConditionalGeneration.from_pretrained(\n            \"Salesforce/blip2-opt-2.7b\",\n            torch_dtype=torch.float16,\n            device_map=\"auto\",\n            low_cpu_mem_usage=True,\n            max_memory={0: \"12GB\"}\n        )\n        print(\"✅ Base model loaded in FP16\")\n        \n        target_modules = []\n        for i in range(32):\n            for proj in [\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\"]:\n                target_modules.append(f\"language_model.model.decoder.layers.{i}.self_attn.{proj}\")\n        \n        lora_config = LoraConfig(\n            task_type=TaskType.SEQ_2_SEQ_LM,\n            r=24,\n            lora_alpha=48,\n            lora_dropout=0.1,\n            bias=\"none\",\n            target_modules=target_modules,\n            init_lora_weights=\"gaussian\"\n        )\n        \n        try:\n            model = get_peft_model(base_model, lora_config)\n            state_dict = checkpoint.get('lora_weights', checkpoint.get('model_state_dict'))\n            if not state_dict:\n                print(\"⚠️ No LoRA weights found in checkpoint\")\n                model = base_model\n            else:\n                compatible_state_dict = {k: v for k, v in state_dict.items() if k in model.state_dict()}\n                model.load_state_dict(compatible_state_dict, strict=False)\n                print(f\"✅ Loaded {len(compatible_state_dict)} LoRA parameters\")\n        \n        except Exception as e:\n            print(f\"⚠️ LoRA loading failed, using base model: {e}\")\n            model = base_model\n        \n        del checkpoint\n        gc.collect()\n        torch.cuda.empty_cache()\n        \n        model = model.to(device)\n        model.eval()\n        \n        print(\"✅ Model loaded successfully\")\n        if torch.cuda.is_available():\n            memory_used = torch.cuda.memory_allocated() / 1024**3\n            print(f\"📊 GPU Memory Used: {memory_used:.2f}GB\")\n        \n        return model, processor\n        \n    except Exception as e:\n        print(f\"❌ Failed to load model: {e}\")\n        for var_name in ['model', 'base_model', 'checkpoint']:\n            if var_name in locals():\n                del locals()[var_name]\n        gc.collect()\n        torch.cuda.empty_cache()\n        raise\n\n# Caption configurations\ndef get_caption_configs():\n    \"\"\"Return three caption generation configurations\"\"\"\n    return [\n        {\n            'name': 'Creative Sampling',\n            'max_length': 80,\n            'num_beams': 1,\n            'do_sample': True,\n            'top_k': 50,\n            'top_p': 0.95,\n            'temperature': 0.9,\n            'no_repeat_ngram_size': 2,\n            'length_penalty': 1.0\n        },\n        {\n            'name': 'Balanced Beam',\n            'max_length': 80,\n            'num_beams': 4,\n            'do_sample': False,\n            'length_penalty': 1.0,\n            'no_repeat_ngram_size': 2,\n            'early_stopping': True\n        },\n        {\n            'name': 'High-Precision Beam',\n            'max_length': 80,\n            'num_beams': 8,\n            'do_sample': False,\n            'length_penalty': 1.0,\n            'no_repeat_ngram_size': 2,\n            'early_stopping': True\n        }\n    ]\n\n# Lightweight scoring for FID\ndef score_caption(caption):\n    \"\"\"Lightweight scoring to optimize FID\"\"\"\n    score = 0.0\n    caption_lower = caption.lower()\n    words = caption.split()\n    word_count = len(words)\n    \n    # Length (FID favors descriptive but concise)\n    if 8 <= word_count <= 20:\n        score += 20.0\n    elif 5 <= word_count <= 25:\n        score += 15.0\n    elif word_count < 5:\n        score -= 10.0\n    \n    # Vocabulary diversity\n    unique_words = len(set(words))\n    diversity_ratio = unique_words / max(word_count, 1)\n    if diversity_ratio > 0.7:\n        score += 15.0\n    elif diversity_ratio < 0.5:\n        score -= 5.0\n    \n    # Keyword presence (common objects/scenes)\n    keywords = ['person', 'player', 'ball', 'car', 'vehicle', 'tree', 'sky', 'building', 'shop', 'sign', 'field']\n    for keyword in keywords:\n        if keyword in caption_lower:\n            score += 10.0\n    \n    # Penalize generic phrases\n    generic_phrases = ['an image', 'a picture', 'this shows', 'visible objects', 'scene']\n    for phrase in generic_phrases:\n        if phrase in caption_lower:\n            score -= 10.0\n    \n    return score\n\n# Clean captions\ndef clean_caption(caption):\n    \"\"\"Enhanced caption cleaning\"\"\"\n    if not caption:\n        return \"An image showing a scene\"\n    \n    caption = re.sub(r'<[^>]*>', '', caption)\n    caption = re.sub(r'\\[.*?\\]', '', caption)\n    caption = re.sub(r'[\\x00-\\x1F\\x7F-\\x9F]', '', caption)\n    caption = re.sub(r'�|', '', caption)\n    \n    problematic_tokens = [\n        'strutconnect', 'strutconnector', 'attrot', 'guiactive', 'guiactiveunfocused',\n        'madeupword', 'confignode', 'partmodule', 'tweakscale', 'modulemanager',\n        'gamedata', 'squad', 'parttools', 'kspfield', 'persistant', 'cfgnode',\n        'guiicon', 'guiname', '0002', 'externaltoevaonly', '裏', 'guiactiveunfocusedmadeupword'\n    ]\n    \n    for token in problematic_tokens:\n        caption = re.sub(rf'\\b{token}\\b', '', caption, flags=re.IGNORECASE)\n    \n    caption = re.sub(r'\\b[A-Z][a-z]*[A-Z][a-zA-Z]*\\b', '', caption)\n    caption = re.sub(r'\\b\\w+000\\d+\\b', '', caption)\n    \n    caption = re.sub(r'\\s+', ' ', caption).strip()\n    caption = re.sub(r'\\s+(and|or|the|a|an)$', '', caption, flags=re.IGNORECASE)\n    caption = re.sub(r'^(the|a|an)\\s+', '', caption, flags=re.IGNORECASE)\n    \n    if caption and len(caption) > 0:\n        caption = caption[0].upper() + caption[1:] if len(caption) > 1 else caption.upper()\n    \n    if len(caption.split()) < 2 or not caption or any(t in caption.lower() for t in problematic_tokens):\n        caption = \"An image showing a scene with visible objects\"\n    \n    return caption\n\n# Post-process captions\ndef post_process_caption(caption):\n    \"\"\"Simplified post-processing\"\"\"\n    return caption.strip()\n\n# Log progress\ndef log_progress(image_id, caption, index, score):\n    \"\"\"Log progress every 100 images with score\"\"\"\n    if index % 100 == 0 or str(index)[:-2] == '100':\n        print(f\"📄 Image {index} (ID: {image_id}) | Caption: '{caption}' | Score: {score:.2f}\")\n\n# Test dataset class\nclass TestDataset(Dataset):\n    def __init__(self, dataframe, processor, test_img_dir):\n        self.dataframe = dataframe\n        self.processor = processor\n        self.test_img_dir = test_img_dir\n    \n    def __len__(self):\n        return len(self.dataframe)\n    \n    def __getitem__(self, idx):\n        row = self.dataframe.iloc[idx]\n        image_id = str(row['image_id'])\n        if not image_id.endswith(('.jpg', '.jpeg', '.png')):\n            image_id += '.jpg'\n        \n        image_path = os.path.join(self.test_img_dir, image_id)\n        try:\n            image = Image.open(image_path).convert('RGB')\n        except Exception as e:\n            print(f\"Error loading image {image_id}: {e}\")\n            image = Image.new('RGB', (224, 224))\n            image_id = f\"{image_id}_error\"\n        \n        return {'image_id': row['image_id'], 'image': image, 'original_image_id': image_id}\n\n# Collate function\ndef collate_fn(batch):\n    images = [item['image'] for item in batch]\n    image_ids = [item['image_id'] for item in batch]\n    \n    inputs = processor(\n        images=images,\n        text=None,\n        padding=\"max_length\",\n        max_length=80,\n        return_tensors=\"pt\",\n        truncation=True\n    )\n    \n    return {\n        'image_ids': image_ids,\n        'pixel_values': inputs['pixel_values'],\n    }\n\n# Main execution\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n\n# Load model and processor\nprint(\"Loading trained model...\")\nmodel, processor = load_trained_model()\n\n# Load test dataset\ntest_df = pd.read_csv(PATHS['test_csv_path'])\nprint(f\"Test dataset loaded: {len(test_df)} samples\")\n\n# Create test dataset and dataloader\ntest_dataset = TestDataset(test_df, processor, PATHS['test_img_dir'])\ntest_dataloader = DataLoader(\n    test_dataset,\n    batch_size=12,  # Increased for speed\n    shuffle=False,\n    collate_fn=collate_fn,\n    num_workers=4,\n    pin_memory=True\n)\n\n# Get configurations\nconfigs = get_caption_configs()\n\n# Generate and score captions\nprint(f\"\\n🎯 Generating captions using three configurations...\")\nmodel.eval()\nimage_ids = []\ncaptions = []\n\nwith torch.no_grad():\n    for batch_idx, batch in enumerate(tqdm(test_dataloader, desc=\"Generating captions\")):\n        pixel_values = batch['pixel_values'].to(device, torch.float16)\n        batch_image_ids = batch['image_ids']\n        \n        batch_captions = []\n        batch_scores = []\n        \n        for config in configs:\n            try:\n                generation_params = {k: v for k, v in config.items() if k != 'name'}\n                generated_ids = model.generate(\n                    pixel_values=pixel_values,\n                    **generation_params\n                )\n                config_captions = processor.batch_decode(generated_ids, skip_special_tokens=True)\n                config_captions = [clean_caption(c) for c in config_captions]\n                config_captions = [post_process_caption(c) for c in config_captions]\n                config_scores = [score_caption(c) for c in config_captions]\n                \n                batch_captions.append(config_captions)\n                batch_scores.append(config_scores)\n                \n            except Exception as e:\n                print(f\"Error with config {config['name']} for batch {batch_image_ids}: {e}\")\n                config_captions = [\"An image showing a scene\" for _ in batch_image_ids]\n                config_scores = [-10.0 for _ in batch_image_ids]\n                batch_captions.append(config_captions)\n                batch_scores.append(config_scores)\n        \n        # Select best caption per image\n        for i in range(len(batch_image_ids)):\n            global_index = batch_idx * test_dataloader.batch_size + i\n            best_caption = \"An image showing a scene\"\n            best_score = -float('inf')\n            \n            for config_idx, (config_captions, config_scores) in enumerate(zip(batch_captions, batch_scores)):\n                if config_scores[i] > best_score:\n                    best_score = config_scores[i]\n                    best_caption = config_captions[i]\n            \n            log_progress(batch_image_ids[i], best_caption, global_index, best_score)\n            image_ids.append(batch_image_ids[i])\n            captions.append(best_caption)\n        \n        torch.cuda.empty_cache()\n\n# Create submission DataFrame\nsubmission_df = pd.DataFrame({\n    'image_id': image_ids,\n    'caption': captions\n})\n\n# Save submission\nsubmission_filename = 'submission_three_configs.csv'\nsubmission_df.to_csv(submission_filename, index=False)\n\nprint(f\"\\n✅ Submission file saved as '{submission_filename}'\")\nprint(f\"📊 Generated {len(submission_df)} captions using best of three configurations\")\n-print(submission_df.head())","metadata":{"id":"zlJinOqEMKz1","outputId":"f804459a-e588-429c-8ab3-b6fc1cd0a77c","trusted":true},"outputs":[],"execution_count":null}]}